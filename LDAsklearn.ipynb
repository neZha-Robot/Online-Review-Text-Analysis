{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn-LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    tword = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        topic_w = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        tword.append(topic_w)\n",
    "        print(topic_w)\n",
    "    return tword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 指定要读取的文件夹路径\n",
    "folder_path = \"data_cut\"\n",
    "# 获取文件夹中的所有xlsx文件\n",
    "file_list = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
    "# 循环读取每个xlsx文件，并对其中的comment列进行操作\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    # 读取Excel文件\n",
    "    data = pd.read_excel(file_path)\n",
    "    # 处理缺失值\n",
    "    data['cut_comment'] = data['cut_comment'].fillna('') \n",
    "    n_features = 1000 #提取1000个特征词语，如果数据量小可以把1000改小，反之同理\n",
    "    tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "    tf = tf_vectorizer.fit_transform(data[\"cut_comment\"])\n",
    "    n_topics = 8#想要生产的主题数\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50,\n",
    "    #                                 doc_topic_prior=0.1,\n",
    "    #                                 topic_word_prior=0.01,\n",
    "                               random_state=0)\n",
    "    lda.fit(tf)\n",
    "    n_top_words = 25#打印每个主题的前25个词语\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    topic_word = print_top_words(lda, tf_feature_names, n_top_words)\n",
    "    topics=lda.transform(tf)\n",
    "    topic = []\n",
    "    for t in topics:\n",
    "        topic.append(list(t).index(np.max(t)))\n",
    "    data['topic']=topic\n",
    "    data.to_excel(\"data_topic.xlsx\",index=False)\n",
    "    topics[0]#0 1 2 \n",
    "    plexs = []\n",
    "    scores = []\n",
    "    n_max_topics = 16#最大主题数+1，用来检验的可以设置大一点\n",
    "    for i in range(1,n_max_topics):\n",
    "        print(i)\n",
    "        lda = LatentDirichletAllocation(n_components=i, max_iter=50,\n",
    "                                    learning_method='batch',\n",
    "                                    learning_offset=50,random_state=0)\n",
    "        lda.fit(tf)\n",
    "        plexs.append(lda.perplexity(tf))\n",
    "        scores.append(lda.score(tf))\n",
    "        n_t=15#区间最右侧的值。注意：不能大于n_max_topics\n",
    "    x=list(range(1,n_t))\n",
    "    plt.plot(x,plexs[1:n_t])\n",
    "    plt.xlabel(\"number of topics\")\n",
    "    plt.ylabel(\"perplexity\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "303.324px",
    "left": "114px",
    "top": "110.322px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "f6922a9521a6ee1ce7f8c1dd0f269e02385237cdfa43bf19c0dcc609bd325874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
